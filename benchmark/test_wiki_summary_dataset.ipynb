{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "%pip install datasets pandas\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import psutil\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "ds = load_dataset(\"jordiclive/wikipedia-summary-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to clean the text by removing special characters and converting to lowercase\n",
    "def clean_text(example):\n",
    "    for key, value in example.items():\n",
    "        if isinstance(value, str):\n",
    "            value = value.replace('\\n', ' ').replace('\\t', ' ')  # Replace newlines with spaces\n",
    "            value = re.sub(r'[^a-zA-Z0-9\\s]', '', value)  # Remove special characters\n",
    "            example[key] = value.lower()  # Convert to lowercase\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess to remove all empty, special characters and convert to lowercase\n",
    "def preprocess_data(ds):\n",
    "    # Remove rows with any None or NaN values in 'full_text' column\n",
    "    ds = ds.filter(lambda x: x['full_text'] is not None and x['full_text'] == x['full_text'])\n",
    "\n",
    "    # Remove special characters and convert to lowercase\n",
    "    ds = ds.map(clean_text)\n",
    "\n",
    "    # Convert to pandas DataFrame\n",
    "    return ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first row to estimate memory usage\n",
    "first_row = ds['train'][0]\n",
    "estimated_row_size = sys.getsizeof(first_row)\n",
    "total_rows = ds['train'].num_rows\n",
    "\n",
    "# Find available memory to either load the entire dataset or a subset\n",
    "available_memory = psutil.virtual_memory().available\n",
    "estimated_memory_required = estimated_row_size * total_rows\n",
    "available_on_disk_memory = psutil.disk_usage('/').free\n",
    "MEM_THRESHOLD = 1024**3 // 2  # 0.5 GB, threshold for loading by chunks\n",
    "\n",
    "print(f\"Estimated memory required: {estimated_memory_required} bytes ~ {estimated_memory_required / 1024**3} GB\")\n",
    "print(f\"Available RAM: {available_memory} bytes ~ {available_memory / 1024**3} GB\")\n",
    "print(f\"Available on-disk memory: {available_on_disk_memory} bytes ~ {available_on_disk_memory / 1024**3} GB\")\n",
    "\n",
    "if estimated_memory_required >= available_on_disk_memory:\n",
    "    print(\"Estimated memory required exceeds available on-disk memory. Exiting.\")\n",
    "    sys.exit()\n",
    "\n",
    "load_by_chunks = False\n",
    "if estimated_memory_required >= min(available_memory, MEM_THRESHOLD):\n",
    "    load_by_chunks = True\n",
    "    print(\"Estimated memory required exceeds available memory. Loading by chunks...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to a CSV file\n",
    "CHUNK_SIZE = 100000 # 100k rows/chunk\n",
    "save_file = 'processed.csv'\n",
    "\n",
    "print(f'Total rows: {total_rows}')\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "if load_by_chunks:\n",
    "    # Process the dataset in chunks\n",
    "    for i in range(0, total_rows, CHUNK_SIZE):\n",
    "        print(f'Processing chunk {i // CHUNK_SIZE + 1} of {total_rows // CHUNK_SIZE + 1}...')\n",
    "        chunk = ds['train'].select(range(i, min(i + CHUNK_SIZE, total_rows)))\n",
    "        \n",
    "        df = preprocess_data(chunk)\n",
    "        \n",
    "        # Save the chunk to a CSV file\n",
    "        if i == 0:\n",
    "            # Write the header for the first chunk\n",
    "            df.to_csv(save_file, index=False, mode='w')\n",
    "        else:\n",
    "            # Append without writing the header for subsequent chunks\n",
    "            df.to_csv(save_file, index=False, mode='a', header=False)\n",
    "else:\n",
    "    # Process the entire dataset at once\n",
    "    df = preprocess_data(ds['train'])\n",
    "    df.to_csv(save_file, index=False)\n",
    "    \n",
    "print(f'Finished processing in {time.perf_counter() - start_time} seconds ~ {(time.perf_counter() - start_time) / 60} minutes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logosdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
