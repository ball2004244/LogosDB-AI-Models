{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/tam/miniconda3/envs/logosdb/lib/python3.11/site-packages (2.21.0)\n",
      "Requirement already satisfied: pandas in /home/tam/miniconda3/envs/logosdb/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /home/tam/miniconda3/envs/logosdb/lib/python3.11/site-packages (from datasets) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/tam/miniconda3/envs/logosdb/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/tam/miniconda3/envs/logosdb/lib/python3.11/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/tam/miniconda3/envs/logosdb/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/tam/miniconda3/envs/logosdb/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/tam/miniconda3/envs/logosdb/lib/python3.11/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /home/tam/miniconda3/envs/logosdb/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/tam/miniconda3/envs/logosdb/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /home/tam/miniconda3/envs/logosdb/lib/python3.11/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /home/tam/miniconda3/envs/logosdb/lib/python3.11/site-packages (from datasets) (3.10.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /home/tam/miniconda3/envs/logosdb/lib/python3.11/site-packages (from datasets) (0.24.5)\n",
      "Requirement already satisfied: packaging in /home/tam/miniconda3/envs/logosdb/lib/python3.11/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/tam/miniconda3/envs/logosdb/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/tam/miniconda3/envs/logosdb/lib/python3.11/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/tam/miniconda3/envs/logosdb/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/tam/miniconda3/envs/logosdb/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/tam/miniconda3/envs/logosdb/lib/python3.11/site-packages (from aiohttp->datasets) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/tam/miniconda3/envs/logosdb/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/tam/miniconda3/envs/logosdb/lib/python3.11/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/tam/miniconda3/envs/logosdb/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/tam/miniconda3/envs/logosdb/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/tam/miniconda3/envs/logosdb/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/tam/miniconda3/envs/logosdb/lib/python3.11/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/tam/miniconda3/envs/logosdb/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tam/miniconda3/envs/logosdb/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tam/miniconda3/envs/logosdb/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tam/miniconda3/envs/logosdb/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tam/miniconda3/envs/logosdb/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tam/miniconda3/envs/logosdb/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Install the required packages\n",
    "%pip install datasets pandas\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import psutil\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "ds = load_dataset(\"jordiclive/wikipedia-summary-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess to remove all empty, special characters and convert to lowercase\n",
    "# Helper function to clean the text by removing special characters and converting to lowercase\n",
    "def clean_text(example):\n",
    "    for key, value in example.items():\n",
    "        if isinstance(value, str):\n",
    "            value = value.replace('\\n', ' ').replace('\\t', ' ')  # Replace newlines with spaces\n",
    "            value = re.sub(r'[^a-zA-Z0-9\\s]', '', value)  # Remove special characters\n",
    "            example[key] = value.lower()  # Convert to lowercase\n",
    "    return example\n",
    "    \n",
    "def preprocess_data(ds):\n",
    "    # Print the number of rows before filtering\n",
    "    print(f\"Rows before filtering: {ds.num_rows}\")\n",
    "\n",
    "    # Initialize a list to store the first 5 rows with missing values\n",
    "    rows_with_missing_values = []\n",
    "\n",
    "    # Iterate through the dataset and collect the first 5 rows with missing values\n",
    "    for row in ds:\n",
    "        if any(value is None or value != value for value in row.values()):\n",
    "            rows_with_missing_values.append(row)\n",
    "        if len(rows_with_missing_values) >= 5:\n",
    "            break\n",
    "\n",
    "    # Convert the subset to a pandas DataFrame and print the first 5 rows\n",
    "    sub_df_pd = pd.DataFrame(rows_with_missing_values)\n",
    "    print(\"First 5 rows with missing values:\")\n",
    "    print(sub_df_pd)\n",
    "    \n",
    "    # Remove rows with any None or NaN values\n",
    "    ds = ds.filter(lambda x: all(value is not None and value == value for value in x.values()))\n",
    "    \n",
    "    # Print the number of rows after filtering\n",
    "    print(f\"Rows after filtering: {ds.num_rows}\")\n",
    "    \n",
    "    # Remove special characters and convert to lowercase\n",
    "    ds = ds.map(clean_text)\n",
    "\n",
    "    # Convert to pandas DataFrame\n",
    "    return ds.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(ds):\n",
    "    # Print the number of rows before filtering\n",
    "    print(f\"Rows before filtering: {ds.num_rows}\")\n",
    "    \n",
    "    # Initialize a list to store the first 5 rows with missing values\n",
    "    rows_with_missing_values = []\n",
    "    \n",
    "    # Iterate through the dataset and collect the first 5 rows with missing values\n",
    "    for row in ds:\n",
    "        if any(value is None or value != value for value in row.values()):\n",
    "            rows_with_missing_values.append(row)\n",
    "        if len(rows_with_missing_values) >= 5:\n",
    "            break\n",
    "    \n",
    "    # Convert the subset to a pandas DataFrame and print the first 5 rows\n",
    "    sub_df_pd = pd.DataFrame(rows_with_missing_values)\n",
    "    print(\"First 5 rows with missing values:\")\n",
    "    print(sub_df_pd)\n",
    "    \n",
    "    # Remove rows with any None or NaN values\n",
    "    ds = ds.filter(lambda x: all(value is not None and value == value for value in x.values()))\n",
    "    \n",
    "    # Print the number of rows after filtering\n",
    "    print(f\"Rows after filtering: {ds.num_rows}\")\n",
    "    \n",
    "    # Remove special characters and convert to lowercase\n",
    "    ds = ds.map(clean_text)\n",
    "\n",
    "    # Convert to pandas DataFrame\n",
    "    return ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(ds):\n",
    "    # Print the number of rows before filtering\n",
    "    print(f\"Rows before filtering: {ds.num_rows}\")\n",
    "    \n",
    "    # Identify columns with missing values\n",
    "    missing_values = {col: sum(1 for x in ds[col] if x is None or x != x) for col in ds.column_names}\n",
    "    print(f\"Missing values per column: {missing_values}\")\n",
    "    \n",
    "    # Create a subset DataFrame with rows that have missing values\n",
    "    sub_df = ds.filter(lambda x: any(value is None or value != value for value in x.values()))\n",
    "    \n",
    "    # Convert the subset to a pandas DataFrame and print the first 5 rows\n",
    "    sub_df_pd = sub_df.to_pandas()\n",
    "    print(\"First 5 rows with missing values:\")\n",
    "    print(sub_df_pd.head())\n",
    "    \n",
    "    # Remove rows with any None or NaN values\n",
    "    ds = ds.filter(lambda x: all(value is not None and value == value for value in x.values()))\n",
    "    \n",
    "    # Print the number of rows after filtering\n",
    "    print(f\"Rows after filtering: {ds.num_rows}\")\n",
    "    \n",
    "    # Remove special characters and convert to lowercase\n",
    "    ds = ds.map(clean_text)\n",
    "\n",
    "    # Convert to pandas DataFrame\n",
    "    return ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(ds):\n",
    "    # Print the number of rows before filtering\n",
    "    print(f\"Rows before filtering: {ds.num_rows}\")\n",
    "    \n",
    "    # Identify columns with missing values\n",
    "    missing_values = {col: sum(1 for x in ds[col] if x is None or x != x) for col in ds.column_names}\n",
    "    print(f\"Missing values per column: {missing_values}\")\n",
    "    \n",
    "    # Create a subset DataFrame with rows that have missing values\n",
    "    sub_df = ds.filter(lambda x: any(value is None or value != value for value in x.values()))\n",
    "    \n",
    "    # Convert the subset to a pandas DataFrame and print the first 5 rows\n",
    "    sub_df_pd = sub_df.to_pandas()\n",
    "    print(\"First 5 rows with missing values:\")\n",
    "    print(sub_df_pd.head())\n",
    "    \n",
    "    # Remove rows with any None or NaN values\n",
    "    ds = ds.filter(lambda x: all(value is not None and value == value for value in x.values()))\n",
    "    \n",
    "    # Print the number of rows after filtering\n",
    "    print(f\"Rows after filtering: {ds.num_rows}\")\n",
    "    \n",
    "    # Remove special characters and convert to lowercase\n",
    "    ds = ds.map(clean_text)\n",
    "\n",
    "    # Convert to pandas DataFrame\n",
    "    return ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated memory required: 1426001288 bytes ~ 1.3280671909451485 GB\n",
      "Available RAM: 61497024512 bytes ~ 57.27356719970703 GB\n",
      "Available on-disk memory: 516082073600 bytes ~ 480.6388854980469 GB\n",
      "Estimated memory required exceeds available memory. Loading by chunks...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the first row to estimate memory usage\n",
    "first_row = ds['train'][0]\n",
    "estimated_row_size = sys.getsizeof(first_row)\n",
    "total_rows = ds['train'].num_rows\n",
    "\n",
    "# Find available memory to either load the entire dataset or a subset\n",
    "available_memory = psutil.virtual_memory().available\n",
    "estimated_memory_required = estimated_row_size * total_rows\n",
    "available_on_disk_memory = psutil.disk_usage('/').free\n",
    "MEM_THRESHOLD = 1024**3  # 1 GB, threshold for loading by chunks\n",
    "\n",
    "print(f\"Estimated memory required: {estimated_memory_required} bytes ~ {estimated_memory_required / 1024**3} GB\")\n",
    "print(f\"Available RAM: {available_memory} bytes ~ {available_memory / 1024**3} GB\")\n",
    "print(f\"Available on-disk memory: {available_on_disk_memory} bytes ~ {available_on_disk_memory / 1024**3} GB\")\n",
    "\n",
    "if estimated_memory_required >= available_on_disk_memory:\n",
    "    print(\"Estimated memory required exceeds available on-disk memory. Exiting.\")\n",
    "    sys.exit()\n",
    "\n",
    "load_by_chunks = False\n",
    "if estimated_memory_required >= min(available_memory, MEM_THRESHOLD):\n",
    "    load_by_chunks = True\n",
    "    print(\"Estimated memory required exceeds available memory. Loading by chunks...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 7750007\n",
      "Processing chunk 1 of 78...\n",
      "Rows before filtering: 100000\n",
      "Missing values per column: {'title': 0, 'description': 0, 'summary': 0, 'full_text': 0, '__index_level_0__': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 100000/100000 [00:00<00:00, 104452.98 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows with missing values:\n",
      "Empty DataFrame\n",
      "Columns: [title, description, summary, full_text, __index_level_0__]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 100000/100000 [00:00<00:00, 102940.97 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after filtering: 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100000/100000 [00:25<00:00, 3953.53 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 2 of 78...\n",
      "Rows before filtering: 100000\n",
      "Missing values per column: {'title': 0, 'description': 0, 'summary': 0, 'full_text': 0, '__index_level_0__': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 100000/100000 [00:00<00:00, 152003.28 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows with missing values:\n",
      "Empty DataFrame\n",
      "Columns: [title, description, summary, full_text, __index_level_0__]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 100000/100000 [00:00<00:00, 148423.65 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after filtering: 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100000/100000 [00:16<00:00, 6117.69 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 3 of 78...\n",
      "Rows before filtering: 100000\n",
      "Missing values per column: {'title': 0, 'description': 0, 'summary': 0, 'full_text': 0, '__index_level_0__': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 100000/100000 [00:00<00:00, 165816.45 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows with missing values:\n",
      "Empty DataFrame\n",
      "Columns: [title, description, summary, full_text, __index_level_0__]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 100000/100000 [00:00<00:00, 169326.24 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after filtering: 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100000/100000 [00:13<00:00, 7410.74 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 4 of 78...\n",
      "Rows before filtering: 100000\n",
      "Missing values per column: {'title': 0, 'description': 0, 'summary': 0, 'full_text': 0, '__index_level_0__': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 100000/100000 [00:00<00:00, 190227.36 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows with missing values:\n",
      "Empty DataFrame\n",
      "Columns: [title, description, summary, full_text, __index_level_0__]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 100000/100000 [00:00<00:00, 186137.70 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after filtering: 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100000/100000 [00:12<00:00, 8310.75 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Save data to a CSV file\n",
    "CHUNK_SIZE = 100000\n",
    "save_file = 'temp.csv'\n",
    "\n",
    "\n",
    "print(f'Total rows: {total_rows}')\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "if load_by_chunks:\n",
    "    # Process the dataset in chunks\n",
    "    for i in range(0, total_rows, CHUNK_SIZE):\n",
    "        print(f'Processing chunk {i // CHUNK_SIZE + 1} of {total_rows // CHUNK_SIZE + 1}...')\n",
    "        chunk = ds['train'].select(range(i, min(i + CHUNK_SIZE, total_rows)))\n",
    "        \n",
    "        df = preprocess_data(chunk)\n",
    "        \n",
    "        # Save the chunk to a CSV file\n",
    "        if i == 0:\n",
    "            # Write the header for the first chunk\n",
    "            df.to_csv(save_file, index=False, mode='w')\n",
    "        else:\n",
    "            # Append without writing the header for subsequent chunks\n",
    "            df.to_csv(save_file, index=False, mode='a', header=False)\n",
    "else:\n",
    "    # Process the entire dataset at once\n",
    "    df = preprocess_data(ds['train'])\n",
    "    df.to_csv(save_file, index=False)\n",
    "    \n",
    "print(f'Finished processing in {time.perf_counter() - start_time} seconds ~ {(time.perf_counter() - start_time) / 60} minutes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logosdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
